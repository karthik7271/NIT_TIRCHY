{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e828eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sarvamai in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (0.1.0)\n",
      "Requirement already satisfied: httpx>=0.21.2 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from sarvamai) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from sarvamai) (2.11.4)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from sarvamai) (2.33.2)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from sarvamai) (4.13.2)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from httpx>=0.21.2->sarvamai) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from httpx>=0.21.2->sarvamai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from httpx>=0.21.2->sarvamai) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from httpx>=0.21.2->sarvamai) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.21.2->sarvamai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from pydantic>=1.9.2->sarvamai) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from pydantic>=1.9.2->sarvamai) (0.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from anyio->httpx>=0.21.2->sarvamai) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from requests) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (3.5.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from datasets) (2.2.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install sarvamai\n",
    "%pip install requests\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a9b1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55599f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NIT_TRICHY/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import base64\n",
    "import wave\n",
    "dataset = load_dataset(\"opus_books\", \"en-fr\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f308be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = dataset.filter(lambda x: 200 < len(x['translation']['en']) < 500)\n",
    "input_block=[]\n",
    "block_size=5\n",
    "for i in range(block_size):\n",
    "    input_block.append(paragraphs[i][\"translation\"][\"en\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c934d2",
   "metadata": {},
   "source": [
    "# MACHINE TRANSLATION API FROM SARVAM AI (MAYURA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94a314b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d283a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 400, {\"error\":{\"request_id\":\"20250505_0e27b2d4-2aa7-4529-bb27-9582ab2bf3c5\",\"message\":\"Source and target languages must be different.\",\"code\":\"invalid_request_error\"}}\n"
     ]
    }
   ],
   "source": [
    "from sarvamai import SarvamAI\n",
    "\n",
    "url = \"https://api.sarvam.ai/translate\"\n",
    "headers = {\n",
    "    \"api-subscription-key\": api_key,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "langs={\"en-IN\":\"English\",\n",
    "\"hi-IN\" : \"Hindi\",\n",
    "\"bn-IN\" : \"Bengali\",\n",
    "\"gu-IN\" : \"Gujarati\",\n",
    "\"kn-IN\" : \"Kannada\",\n",
    "\"ml-IN\": \"Malayalam\",\n",
    "\"mr-IN\": \"Marathi\",\n",
    "\"od-IN\": \"Odia\",\n",
    "\"pa-IN\": \"Punjabi\",\n",
    "\"ta-IN\": \"Tamil\",\n",
    "\"te-IN\": \"Telugu\"}\n",
    "\n",
    "result=[]\n",
    "for i in range(1):\n",
    "    for j in langs:\n",
    "        payload = {\n",
    "        \"source_language_code\": \"en-IN\",\n",
    "        \"target_language_code\":j,\n",
    "        \"speaker_gender\": \"Male\",\n",
    "        \"mode\": \"classic-colloquial\",\n",
    "        \"model\": \"mayura:v1\",\n",
    "        \"enable_preprocessing\": False,\n",
    "        \"input\": input_block[i]}\n",
    "        \n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            translated_text = response.json().get(\"translated_text\", \"Translation not available\")\n",
    "            result.append(translated_text)\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1bc78",
   "metadata": {},
   "source": [
    "### ERROR OCCURED BECAUSE BOTH SOURCE AND TARGET CODE WHERE THE SAME (en-IN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac98651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"बगीचे में सेब चुराने वाले बड़े ही चुपचाप मेंढक, घेरे के छेद से निकल गए... मेरी माँ, जिन्हें हम Millie बुलाते थे और जो घर की सबसे बढ़िया सेविका थीं, उन्होंने तुरंत ही सारे कमरों में धूल भरी गद्दी भर दी और बहुत घबराहट में - जैसा कि हर 'removal' में करती थीं - बताया कि हमारा furniture इतने घटिया घर में बिलकुल नहीं जमेगा।\",\n",
       " 'বাগানের urchin-গুলো চুপচাপ hedge-এর ছিদ্র দিয়ে peach চুরি করছিল। আমার মা, যাকে আমরা Millie বলে ডাকতাম, খুবই methodical housekeeper ছিলেন। তিনি  তাড়াতাড়ি  straw-এ ভরা ঘরগুলোতে ঢুকে  ঘটনাটা  ঘটনা করে  বলে  দেন,  যে  বাড়িটা  এত  খারাপ  বানানো,  এখানে  আমাদের furniture-গুলো  কখনোই  ঠিকমতো  fit  হবে  না।',\n",
       " \"બગીચામાં peaches ચોરતા urchins hedge-ના કાણાંમાંથી શાંતિથી ભાગી ગયા. મારી મમ્મી, જેમને અમે Millie કહેતા અને જે ખૂબ જ વ્યવસ્થિત housekeeper હતાં, એ તરત જ બધા rooms-માં ધૂળથી ભરેલા straw-માં ગયાં અને બહુ ગભરાઈને - જેમ કે એમનો દરેક 'removal'-માં રિવાજ હતો - કીધું કે આટલા ખરાબ રીતે બનાવેલા ઘરમાં અમારું furniture બરાબર નહિ બેસે.\",\n",
       " \"ತೋಟದಲ್ಲಿ peaches ಕದ್ದಿದ್ದ urchins, hedge-ನಲ್ಲಿರೋ ರಂಧ್ರಗಳ ಮೂಲಕ ಸದ್ದಿಲ್ಲದೆ ತಪ್ಪಿಸಿಕೊಂಡವು. ನಾವು Millie ಅಂತ ಕರೀತಿದ್ದ ನಮ್ಮ ತಾಯಿ, ತುಂಬಾ ಶಿಸ್ತಿನ ಮನೆಯವರಾಗಿದ್ದರು. ಅವರು ತಕ್ಷಣ straw ತುಂಬಿದ್ದ ಕೋಣೆಗಳ ಒಳಗೆ ಹೋಗಿ, ತುಂಬಾ ಭಯದಿಂದ - ಪ್ರತಿ 'removal'-ನಲ್ಲೂ ಅವರದ್ದೇ ಆದ ರೀತಿ - ನಮ್ಮ furniture ಇಲ್ಲಿ ಸರಿ ಹೋಗಲ್ಲ ಅಂತ ಹೇಳಿದ್ರು.\",\n",
       " \"തോട്ടത്തിലെ പീച്ചുകൾ മോഷ്ടിക്കുന്ന urchins, hedge-ലെ ദ്വാരങ്ങളിലൂടെ നിശബ്ദമായി രക്ഷപ്പെട്ടു... ഞങ്ങൾ Millie എന്ന് വിളിച്ചിരുന്ന അമ്മ, എനിക്കറിയാവുന്ന ഏറ്റവും methodical housekeeper ആയിരുന്നു. അവർ ഉടനെ പൊടിപടലങ്ങളാൽ നിറഞ്ഞ മുറികളിൽ കയറി, വളരെ ആശ്ചര്യത്തോടെ - ഓരോ 'removal'-ലും അവരുടെ പതിവ് പോലെ - ഞങ്ങളുടെ furniture ഇത്ര മോശമായി നിർമ്മിച്ച വീട്ടിൽ ഒരിക്കലും ചേരില്ലെന്ന് പ്രഖ്യാപിച്ചു.\",\n",
       " \"बागेत peaches चोरणारे Urchins hedge-मधल्या दऱ्यांमधून शांतपणे निघून गेले. . . माझी आई, ज्यांना आम्ही Millie म्हणायचो आणि ज्या खूप व्यवस्थित घरकाम करायच्या, त्या लगेच धुळीने भरलेल्या खोल्यांमध्ये गेल्या आणि खूप भीतीने त्यांनी - जसं की प्रत्येक 'removal'-मध्ये त्यांच्याकडून केले जायचे - सांगितलं की इतक्या वाईट पद्धतीने बांधलेल्या घरात आमचे furniture कधीच जुळणार नाही.\",\n",
       " \"ଉଦ୍ୟାନରେ ଅର୍ଚ୍ଚିନ୍\\u200cମାନେ ଚୁପ୍\\u200cଚାପ୍ peach ଚୋରି କରୁଥିବା ବେଳେ hedge-ର ଛିଦ୍ର ଦେଇ ପଳାଇଗଲେ। ଆମେ ତାଙ୍କୁ Millie ଡାକୁଥିଲୁ ଏବଂ ସେ ସବୁଠୁ ଭଲ housekeeper ଥିଲୁ। ସେ ତୁରନ୍ତ straw ଭର୍ତି ରୁମ୍\\u200cଗୁଡ଼ିକରେ ପଶିଗଲେ ଏବଂ ବହୁତ ଡରି ଡରି କହିଲେ - ଯେମିତି ପ୍ରତ୍ୟେକ 'removal' ବେଳେ ସେ କରୁଥିଲେ - ଯେ ଏତେ ଖରାପ ଭାବରେ ତିଆରି ଘର ଆମ furniture-କୁ ଫିଟ୍ ହେବନି।\",\n",
       " \"ਬਾਗ਼ ਵਿੱਚ peaches ਚੋਰੀ ਕਰਨ ਵਾਲੇ urchins hedge ਦੇ holes ਵਿੱਚੋਂ ਚੁੱਪ-ਚਾਪ ਭੱਜ ਗਏ। ਮੇਰੀ ਮਾਂ, ਜਿਸਨੂੰ ਅਸੀਂ Millie ਕਹਿੰਦੇ ਸੀ ਅਤੇ ਜੋ ਇੱਕ ਬਹੁਤ ਹੀ methodical housekeeper ਸੀ, ਉਸਨੇ ਤੁਰੰਤ straw ਨਾਲ ਭਰੇ ਕਮਰਿਆਂ ਵਿੱਚ ਵੜ ਗਈ ਅਤੇ ਬਹੁਤ ਘਬਰਾਹਟ ਨਾਲ ਐਲਾਨ ਕਰ ਦਿੱਤਾ - ਜਿਵੇਂ ਕਿ ਹਰ 'removal' 'ਤੇ ਕਰਦੀ ਹੁੰਦੀ ਸੀ - ਕਿ ਸਾਡਾ furniture ਇਸ ਮਾੜੇ ਬਣੇ ਘਰ ਵਿੱਚ ਕਦੇ ਵੀ ਨਹੀਂ ਫਿੱਟ ਹੋਵੇਗਾ।\",\n",
       " 'தோட்டம்ல peach-கள திருடிட்டு இருந்த urchins, hedge-ல இருக்கற துளைகள் வழியா அமைதியா ஓடிப் போயிட்டாங்க.  நம்ம எல்லாரும் Millie-ன்னு கூப்பிட்ட எங்க அம்மா, ரொம்ப ஒழுங்கா வீட்டு வேலை பாப்பாங்க. அவங்க உடனே எல்லா ரூம்லயும் தூசி படிஞ்ச straw-ல போய், ரொம்ப பயத்தோட, \"எங்க furniture இந்த மாதிரி சரியில்லாத வீட்ல பொருந்தாது\"ன்னு சொல்லிட்டாங்க.',\n",
       " 'మా తోటలో peaches దొంగిలించే urchins hedge-లోని రంధ్రాల ద్వారా నిశ్శబ్దంగా పారిపోయారు. మేము Millie అని పిలిచే మా అమ్మగారు చాలా క్రమశిక్షణతో ఇంటిని చక్కబెట్టేవారు. ఆవిడ వెంటనే straw-తో నిండిన గదుల్లోకి వెళ్ళి, \"మా furniture ఇంత చెడ్డగా నిర్మించిన ఇంట్లో సరిపోదు\" అని చాలా భయంగా చెప్పారు.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bd836c",
   "metadata": {},
   "source": [
    "# LANGUAGE IDENTIFICATION FROM SARVAM AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "efb82f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Detection Results ===\n",
      "Detected Language Code: te-IN\n",
      "Detected Script Code: Telu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.sarvam.ai/text-lid\"\n",
    "\n",
    "headers = {\n",
    "    \"api-subscription-key\": api_key,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "example_text=\"పిలిచే మా అమ్మగారు చాలా క్రమశిక్షణతో ఇంటిని చక్కబెట్టేవారు.\"\n",
    "\n",
    "payload = {\n",
    "    \"input\": example_text\n",
    "}\n",
    "\n",
    "# Send API request\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# Process response\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    language_code = data.get(\"language_code\", \"Language not detected\")\n",
    "    script_code = data.get(\"script_code\", \"Script not detected\")\n",
    "\n",
    "    print(\"\\n=== Detection Results ===\")\n",
    "    print(f\"Detected Language Code: {language_code}\")\n",
    "    print(f\"Detected Script Code: {script_code}\\n\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a3ebd",
   "metadata": {},
   "source": [
    "CASE WHERE WE GET NO LANGUAGE/SCRIPT DETECTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5320d85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Detection Results ===\n",
      "Detected Language Code: None\n",
      "Detected Script Code: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_text=\"A'in jun aatinob'aal li maare ink'a' neketaw ru.\"\n",
    "\n",
    "payload = {\n",
    "    \"input\": example_text\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    language_code = data.get(\"language_code\", \"Language not detected\")\n",
    "    script_code = data.get(\"script_code\", \"Script not detected\")\n",
    "\n",
    "    print(\"\\n=== Detection Results ===\")\n",
    "    print(f\"Detected Language Code: {language_code}\")\n",
    "    print(f\"Detected Script Code: {script_code}\\n\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba285d9",
   "metadata": {},
   "source": [
    "# TEXT TO SPEECH USING SARVAM AI API (BULBUL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aea8f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.sarvam.ai/text-to-speech\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-subscription-key\": api_key \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1490ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file 0 saved successfully as 'output1.wav'!\n",
      "Audio file 1 saved successfully as 'output2.wav'!\n",
      "Audio file 2 saved successfully as 'output3.wav'!\n",
      "Audio file 3 saved successfully as 'output4.wav'!\n",
      "Audio file 4 saved successfully as 'output5.wav'!\n"
     ]
    }
   ],
   "source": [
    "for i in range(block_size):\n",
    "    input_text=input_block[i]\n",
    "    payload = {\n",
    "            \"inputs\": [input_text],\n",
    "            \"target_language_code\": \"ta-IN\",  \n",
    "            \"speaker\": \"neel\",  # Speaker voice\n",
    "            \"model\": \"bulbul:v1\",  # Model to use\n",
    "            \"pitch\": 0,  # Pitch adjustment\n",
    "            \"pace\": 1.0,  # Speed of speech\n",
    "            \"loudness\": 1.0,  # Volume adjustment\n",
    "            \"enable_preprocessing\": True,  # Enable text preprocessing\n",
    "        }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    \n",
    "    #SAVING THE RESPONSES OBTAINED IN AUDIO FILES\n",
    "    if response.status_code == 200:\n",
    "        audio = response.json()[\"audios\"][0]\n",
    "        \n",
    "        audio = base64.b64decode(audio)\n",
    "\n",
    "        with wave.open(f\"output{i}.wav\", \"wb\") as wav_file:\n",
    "                # Set the parameters for the .wav file\n",
    "            wav_file.setnchannels(1)  # Mono audio\n",
    "            wav_file.setsampwidth(2)  # 2 bytes per sample\n",
    "            wav_file.setframerate(22050)  # Sample rate of 22050 Hz\n",
    "                # Write the audio data to the file\n",
    "            wav_file.writeframes(audio)\n",
    "        print(f\"Audio file {i} saved successfully as 'output{i+1}.wav'!\")\n",
    "    else:\n",
    "        print(f\"Error for chunk {i}: {response.status_code}\")\n",
    "        print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f1d576",
   "metadata": {},
   "source": [
    "# CALL ANALYTICS USING SARVAM AI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800d5533",
   "metadata": {},
   "source": [
    "### TESTING ON A CONVERSIONAL AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c658ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"test.wav\"\n",
    "\n",
    "questions=[\n",
    "    {\n",
    "        \"id\":\"q1\", \n",
    "        \"text\":\"who are the two people conversing in the call?\", \n",
    "        \"type\":\"long answer\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"id\":\"q2\", \n",
    "        \"text\":\"Give me an Sentiment analysis of this call recording\",\n",
    "        \"type\":\"short answer\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"id\":\"q3\" , \n",
    "        \"text\":\"Give me a Short summary of what was spoken in this call\", \n",
    "        \"type\":\"long answer\"\n",
    "    }\n",
    "]\n",
    "\n",
    "headers={\n",
    "    \"api-subscription-key\":api_key,\n",
    "    \"Accept\":\"application/json\"\n",
    "    }\n",
    "\n",
    "\n",
    "with open(audio_path,\"rb\") as audio_file:\n",
    "    files = {\n",
    "\n",
    "        \"file\": (\"harvard.wav\", audio_file, \"audio/wav\"),\n",
    "        \n",
    "        \"questions\":       (None, json.dumps(questions)),\n",
    "        \"model\":           (None, \"saaras:v1\"),\n",
    "        \"with_diarization\":(None, \"false\")\n",
    "    }\n",
    "    url=\"https://api.sarvam.ai/call-analytics\"\n",
    "    \n",
    "    response=requests.post(url,headers=headers,files=files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff1181c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript:  So, Erin, in your email you said you wanted to talk about the exam. Yeah, um, I've just never taken a class with so many different readings. I've managed to keep up with all the assignments, but I'm not sure how to How to how to review everything? Yeah. In other classes I've had, there's usually just one book to review, not three different books, plus all those other text excerpts and videos.\n",
      "who are the two people conversing in the call?==> Erin and the speaker she emailed about the exam.\n",
      "Give me an Sentiment analysis of this call recording==> The call sentiment is neutral to slightly concerned, as Erin discusses worry about managing study materials for an exam.\n",
      "Give me a Short summary of what was spoken in this call==> Erin expressed concern to the speaker about managing various reading materials for an exam and asked for guidance on reviewing them.\n"
     ]
    }
   ],
   "source": [
    "if response.status_code==200:\n",
    "    data = response.json()\n",
    "    print(\"Transcript: \", data.get(\"transcript\"))\n",
    "    for ans in data.get(\"answers\",[]):\n",
    "        text=ans.get(\"response\") or ans.get(\"answer\")\n",
    "        print(f\"{ans[\"question\"]}==> {text}\")\n",
    "else:\n",
    "    print(\"Error\",response.status_code,response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024da5f6",
   "metadata": {},
   "source": [
    "# TEXT ANALYTICS USING SARVAM AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d2064e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"SetFit/20_newsgroups\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97d6224e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \n",
      ": In any case, I think Viola would have made a better signing.  Why?\n",
      ": Viola is younger, and is left handed (how many left handed starters does\n",
      ": Toronto have?\n",
      "\n",
      "Well, I agree that Viola is a better signing.  However, why does\n",
      "everyone say that you want lefthanded starters?  I understand lefthanded\n",
      "spot relievers, even though they usually face more righthanded batters\n",
      "than lefthanded batters.  I just don't understand why people insist\n",
      "on lefthanded starters, unless there is a park effect (e.g., Yankee Stadium).\n",
      "Most batters in MLB are righthanded, so righthanded starters will have\n",
      "the platoon advantage more often than lefthanded starters.\n",
      "I guess one argument for lefty starters is that certain teams\n",
      "may be more vulnerable to LHP's than RHP's.  However, this is probably\n",
      "only a factor in the postseason, because teams seldom juggle their starters\n",
      "for this reason during the regular season.\n",
      "\n",
      "I think you just want the best starters you can get, regardless of\n",
      "whether they are lefties or righties.  Lefthanded starters tend to have\n",
      "higher ERA's than righthanded starters, precisely because managers\n",
      "go out of their way to start inferior lefties (or perhaps because of\n",
      "the platoon advantage).\n",
      "\n",
      "Am I missing something here?\n",
      "Label: 9\n",
      "Label_text: rec.sport.baseball\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[44]\n",
    "print(\"Text:\", sample[\"text\"])\n",
    "print(\"Label:\", sample[\"label\"])\n",
    "print(\"Label_text:\", sample[\"label_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a50a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "questions = [\n",
    "    {\n",
    "        \"id\":   \"q1\",\n",
    "        \"text\": \"Summarize the given text, what topic does it talk about??\",\n",
    "        \"type\": \"long answer\"\n",
    "    },\n",
    "    {\n",
    "        \"id\":   \"q2\",\n",
    "        \"text\": \"Return all the Named Entities from the Text.\",\n",
    "        \"type\": \"long answer\"\n",
    "    },\n",
    "    {\n",
    "        \"id\":   \"q3\",\n",
    "        \"text\": \"Also perform sentinment analysis on the text.\",\n",
    "        \"type\": \"short answer\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"id\":   \"q4\",\n",
    "        \"text\": \"Is this text part of a literature peice.\",\n",
    "        \"type\": \"short answer\"\n",
    "    }\n",
    "]\n",
    "\n",
    "headers = {\n",
    "    \"api-subscription-key\": api_key,\n",
    "    \"Accept\":               \"application/json\"\n",
    "}\n",
    "\n",
    "files = {\n",
    "    \n",
    "    \"text\":      (None, sample[\"text\"]),\n",
    "    \"questions\": (None, json.dumps(questions)),\n",
    "    \"model\":     (None, \"saaras:v1\")\n",
    "}\n",
    "url = \"https://api.sarvam.ai/text-analytics\"\n",
    "response = requests.post(url, headers=headers, files=files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d392e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document Analysis:\n",
      "Summarize the given text, what topic does it talk about?? → The text discusses the merits and rationale behind signing left-handed vs right-handed starting pitchers in baseball, questioning the common preference for left-handed starters.\n",
      "Return all the Named Entities from the Text. → Viola, Toronto, MLB, Yankee Stadium, LHP, RHP, ERA\n",
      "Also perform sentinment analysis on the text. → Neutral\n",
      "Is this text part of a literature peice. → No\n"
     ]
    }
   ],
   "source": [
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"\\nDocument Analysis:\")\n",
    "    for ans in result.get(\"answers\", []):\n",
    "        text = ans.get(\"response\") or ans.get(\"answer\")\n",
    "        print(f\"{ans['question']} → {text}\")\n",
    "else:\n",
    "    print(\"Error\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134676c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NIT_TRICHY",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
